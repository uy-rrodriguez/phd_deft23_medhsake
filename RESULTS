Résultats sur le dev (exact match) :

prompt_0 =>

# modèles off-the-shelf (avec instruction finetuning)
bloomz-560m                0.0737    
bloomz-3b                  0.1442    
bloomz-7b1                 0.1602    
bloomz-7b1-mt              0.1762    
flan-t5-xxl                0.1794    
flan-ul2                   0.1570    
tk-instruct-3b-def         0.1346    
tk-instruct-11b-def        0.1826    
oasst-sft-1-pythia-12b     0.0705
opt-iml-1.3b               0.0673    
opt-iml-30b                0.1442    
galactica-125m             0.0128
galactica-1.2b             0.0192
galactica-6.7b             0.0352
mpt-instruct-7b            0.0641
pmc-llama-7b               0.0224

# trad automatique anglais (pas l'air de marcher)
en/bloomz-3b               0.1153    ??
en/bloomz-560m             0.1442    
en/tk-instruct-11b-def     0.1442    

# modèles llama (+adaptation lora)

(int8)
llama_7B                   0.0576    
llama2-7b                  0.0833
llama2-7b-chat             0.0801
llama_7B+alpaca_fr         0.1185                                                                          
llama_7B+alpaca            0.1217
llama_7B+alpaca-native     0.1153
llama_7B+deft              0.1378    
llama2-7b-deft             0.1410
llama2-7b-deft-noprompt    0.2179
llama_13B                  0.0769    
llama2-13b                 0.1442
llama2-13b-chat            0.1474
llama2-13b-deft            0.2788
llama_13B+alpaca           0.1474
llama_13B+vicuna           0.1538
llama_13B+deft             0.1730    
llama_30B                  0.1442    
llama_30B+alpaca           0.1923
llama_30B+deft             0.2467    
llama_65B                  0.1730    
llama2-70b                 0.2051
llama2-70b-chat            0.2211
llama_65B+deft             0.3044      
llama2-70b-deft            0.4455
llama2-70b-comp            0.4679

(fp16)
llama_30B                  0.1891    
llama_65B                  0.2179    

(api)
openai/code-cushman-001    0.1121    
openai/code-davinci-002    0.3108    
ai21/j1-jumbo              0.0833    
cohere_command-xlarge-beta 0.1057

=> autres prompts
code-cushman-001            0.1346
code-davinci-002_run2       0.3205
code-davinci-002            0.2435
gpt-3.5-turbo-0301.run2     0.4551
gpt-3.5-turbo-0301          0.4038
text-curie-001              0.1217
text-davinci-003            0.2884
gpt-4                       0.7788


FrenchMedMCQA: A French Multiple-Choice Question Answering Dataset for Medical domain
https://hal.science/hal-03824241v2/preview/LOUHI_2022___QA_22.pdf#page=2

                 w/o Context Wiki-BM25   HAL-BM25    Wiki-MiniLMv2 HAL-MiniLMv2
Architecture     Hamming EMR Hamming EMR Hamming EMR Hamming EMR   Hamming EMR
BioBERT_V1.1     36.19 15.43 38.72 16.72 33.33 14.14 35.13 16.23   34.27 13.98
PubMedBERT       33.98 14.14 34.00 13.98 35.66 15.59 33.87 14.79   35.44 14.79
CamemBERT-base   36.24 16.55 34.19 14.46 34.78 15.43 34.66 14.79   34.61 14.95
XLM-RoBERTa-base 37.92 17.20 31.26 11.89 35.84 16.07 32.47 14.63   33.00 14.95
BART-base        31.93 15.91 34.98 18.64 33.80 17.68 29.65 12.86   34.65 18.32


